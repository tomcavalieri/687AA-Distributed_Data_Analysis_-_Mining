{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION OF CRIME SOLVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new Spark Context to use for the execution of the script\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"MY-APP-NAME\", master=\"local[*]\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# importing all the necessary libraries for building and evaluating the implemented models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, Bucketizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlCtx.read.load(\"Homicide_Undersampled.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in ['Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race','Weapon','Victim_Age']:\n",
    "    indexer = StringIndexer(inputCol=attribute, outputCol=attribute+\"_index\")\n",
    "    df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race', 'Victim_Ethnicity','Weapon','Agency_Code', 'Perpetrator_Age','Perpetrator_Sex','Perpetrator_Race','Perpetrator_Ethnicity','Relationship','Record_Source','City_State', 'Record_ID','Victim_Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"Crime_Solved\", outputCol=\"label\")\n",
    "x = indexer.fit(df)\n",
    "df = x.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Crime_Solved',\n",
       " 'Agency_Type_index',\n",
       " 'State_index',\n",
       " 'Month_index',\n",
       " 'Victim_Sex_index',\n",
       " 'Victim_Race_index',\n",
       " 'Weapon_index',\n",
       " 'Victim_Age_index',\n",
       " 'label']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.drop('Crime_Solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_log.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data.map(lambda x: LabeledPoint(x[8], x[0:8])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for calculating evaluations metrics\n",
    "def evaluation(predictions):\n",
    "    print('Accuracy:', MulticlassMetrics(predictions).accuracy)\n",
    "    print('Precision 1.0:', MulticlassMetrics(predictions).precision('1.0'))\n",
    "    print('Precision 0.0:', MulticlassMetrics(predictions).precision('0.0'))\n",
    "    print('Recall 1.0:', MulticlassMetrics(predictions).recall('1.0'))\n",
    "    print('Recall 0.0:', MulticlassMetrics(predictions).recall('0.0'))\n",
    "    print('F1:', MulticlassMetrics(predictions).fMeasure(1.0))\n",
    "    print('Area under PR:', BinaryClassificationMetrics(predictions).areaUnderPR)\n",
    "    print('Area under ROC:', BinaryClassificationMetrics(predictions).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and test (30%)\n",
    "training, test = labeled_data.randomSplit([0.7, 0.3], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5807803757904905\n",
      "Precision 1.0: 0.5691256268687412\n",
      "Precision 0.0: 0.5966130106810513\n",
      "Recall 1.0: 0.6571376381404694\n",
      "Recall 0.0: 0.5047729876175064\n",
      "F1: 0.6099732103496219\n",
      "Area under PR: 0.5570785107449165\n",
      "Area under ROC: 0.5809553128789879\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.12558039236024862\n"
     ]
    }
   ],
   "source": [
    "trainErr = predictionAndLabels.filter(lambda lp: lp[1] != lp[0]).count() / float(data.count()) \n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['Year','Agency_Type_index','State_index', 'Month_index',\n",
    "    'Victim_Sex_index','Victim_Race_index','Weapon_index','Victim_Age_index'],\n",
    "    outputCol=\"features\")\n",
    "df_mlp = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlp = df_mlp.drop('Year',\n",
    "'Victim_Age_index',\n",
    " 'Agency_Type_index',\n",
    " 'State_index',\n",
    " 'Month_index',\n",
    " 'Victim_Sex_index',\n",
    " 'Victim_Race_index',\n",
    " 'Weapon_index', 'Crime_solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[1980.0,0.0,37.0,...|\n",
      "|  0.0|[1980.0,0.0,37.0,...|\n",
      "|  0.0|[1980.0,0.0,37.0,...|\n",
      "|  0.0|[1980.0,0.0,37.0,...|\n",
      "|  0.0|(8,[0,2,3,7],[198...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,3.0,37.0,...|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|(8,[0,1,2],[1980....|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|[1980.0,1.0,18.0,...|\n",
      "|  0.0|(8,[0,2,3,7],[198...|\n",
      "|  0.0|(8,[0,2,3,6],[198...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_mlp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "(training, test) = df_mlp.randomSplit([0.7, 0.3],seed=0)\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [8, 16, 12, 6, 4, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=1000, layers=layers, blockSize=256, seed=0)\n",
    "\n",
    "# train the model\n",
    "model = trainer.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create new evaluators since the others give an error due to the model classifying everything identically\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "\n",
    "# Make predicitons\n",
    "predictionAndTarget = model.transform(test).select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "f1 = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"f1\"})\n",
    "weightedPrecision = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluatorMulti.evaluate(predictionAndTarget, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "auc = evaluator.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5003602206921709\n",
      "F1 score = 0.33373365542258593\n",
      "Weighted Precision = 0.250360350451118\n",
      "Weighted Recall = 0.5003602206921709\n",
      "AUC = 0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \" + str(acc))\n",
    "print(\"F1 score = \" + str(f1))\n",
    "print(\"Weighted Precision = \" + str(weightedPrecision))\n",
    "print(\"Weighted Recall = \" + str(weightedRecall))\n",
    "print(\"AUC = \" + str(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionAndTarget.select('prediction').distinct().show()\n",
    "# as suspected, MLP classifies every record as solved, resulting in performances not better than the random\n",
    "# classifier; therefore this model shouldn't be exploited for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
