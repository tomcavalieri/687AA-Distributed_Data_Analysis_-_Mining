{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION OF CRIME SOLVED ON SUBSAMPLES OF THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new Spark Context to use for the execution of the script\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(appName=\"MY-APP-NAME\", master=\"local[*]\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "\n",
    "# importing all the necessary libraries for building and evaluating the implemented models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel \n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, Bucketizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the already subsampled dataset containing only homicides commited in California\n",
    "df_cali = sqlCtx.read.load(\"Homicide_California.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating index columns\n",
    "for attribute in ['Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race','Weapon','Victim_Age','Crime_Solved']:\n",
    "    indexer = StringIndexer(inputCol=attribute, outputCol=attribute+\"_index\")\n",
    "    df_cali = indexer.fit(df_cali).transform(df_cali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130845"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cali.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cali = df_cali.drop('Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race', 'Victim_Ethnicity','Weapon','Agency_Code', 'Perpetrator_Age','Perpetrator_Sex','Perpetrator_Race','Perpetrator_Ethnicity','Relationship','Record_Source','City_State', 'Record_ID','Victim_Age','Crime_Solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cali = df_cali.rdd #transforming the data into an rdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform each record to a labeled point in order to feed it to the model\n",
    "labeled_data = data_cali.map(lambda x: LabeledPoint(x[8], x[0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and test (30%)\n",
    "training, test = labeled_data.randomSplit([0.7, 0.3], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for calculating evaluations metrics\n",
    "def evaluation(predictions):\n",
    "    print('Accuracy:', MulticlassMetrics(predictions).accuracy)\n",
    "    print('Precision 1.0:', MulticlassMetrics(predictions).precision('1.0'))\n",
    "    print('Precision 0.0:', MulticlassMetrics(predictions).precision('0.0'))\n",
    "    print('Recall 1.0:', MulticlassMetrics(predictions).recall('1.0'))\n",
    "    print('Recall 0.0:', MulticlassMetrics(predictions).recall('0.0'))\n",
    "    print('F1:', MulticlassMetrics(predictions).fMeasure(1.0))\n",
    "    print('Area under PR:', BinaryClassificationMetrics(predictions).areaUnderPR)\n",
    "    print('Area under ROC:', BinaryClassificationMetrics(predictions).areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5654921829021775\n",
      "Precision 1.0: 0.5637057670483855\n",
      "Precision 0.0: 0.5660813937661631\n",
      "Recall 1.0: 0.29995608256477824\n",
      "Recall 0.0: 0.7973160795590702\n",
      "F1: 0.3915582786914616\n",
      "Area under PR: 0.5295447032752114\n",
      "Area under ROC: 0.5486360810619243\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.12977951010737895\n"
     ]
    }
   ],
   "source": [
    "# computing training error\n",
    "trainErr = predictionAndLabels.filter(lambda lp: lp[1] != lp[0]).count() / float(data_cali.count()) \n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York\n",
    "##### Repeting all the previous steps on NY's homicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny = sqlCtx.read.load(\"Homicide_NY.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in ['Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race','Weapon','Victim_Age','Crime_Solved']:\n",
    "    indexer = StringIndexer(inputCol=attribute, outputCol=attribute+\"_index\")\n",
    "    df_ny = indexer.fit(df_ny).transform(df_ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46586"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ny.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ny = df_ny.drop('Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race', 'Victim_Ethnicity','Weapon','Agency_Code', 'Perpetrator_Age','Perpetrator_Sex','Perpetrator_Race','Perpetrator_Ethnicity','Relationship','Record_Source','City_State', 'Record_ID','Victim_Age','Crime_Solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ny = df_ny.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data_ny.map(lambda x: LabeledPoint(x[8], x[0:8])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and test (30%)\n",
    "training, test = labeled_data.randomSplit([0.7, 0.3], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5733132573313258\n",
      "Precision 1.0: 0.542232630757221\n",
      "Precision 0.0: 0.5997083001856272\n",
      "Recall 1.0: 0.534966112138016\n",
      "Recall 0.0: 0.6067069081153588\n",
      "F1: 0.5385748623710941\n",
      "Area under PR: 0.524385517377006\n",
      "Area under ROC: 0.5708365101266875\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.12774224015798738\n"
     ]
    }
   ],
   "source": [
    "trainErr = predictionAndLabels.filter(lambda lp: lp[1] != lp[0]).count() / float(data_ny.count()) \n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texas\n",
    "##### Repeting all the previous steps on Texas' homicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tex = sqlCtx.read.load(\"Homicide_Texas.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in ['Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race','Weapon','Victim_Age','Crime_Solved']:\n",
    "    indexer = StringIndexer(inputCol=attribute, outputCol=attribute+\"_index\")\n",
    "    df_tex = indexer.fit(df_tex).transform(df_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87506"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tex.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tex = df_tex.drop('Agency_Type', 'State', 'Month', 'Victim_Sex', 'Victim_Race', 'Victim_Ethnicity','Weapon','Agency_Code', 'Perpetrator_Age','Perpetrator_Sex','Perpetrator_Race','Perpetrator_Ethnicity','Relationship','Record_Source','City_State', 'Record_ID','Victim_Age','Crime_Solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tex = df_tex.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = data_tex.map(lambda x: LabeledPoint(x[8], x[0:8])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%) and test (30%)\n",
    "training, test = labeled_data.randomSplit([0.7, 0.3], seed=11)\n",
    "training.cache()\n",
    "\n",
    "# Run training algorithm to build the model\n",
    "model = LogisticRegressionWithLBFGS.train(training)\n",
    "\n",
    "# Compute raw scores on the test set\n",
    "predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5219155533437967\n",
      "Precision 1.0: 0.47255619445896496\n",
      "Precision 0.0: 0.5303852536215634\n",
      "Recall 1.0: 0.1472432608518609\n",
      "Recall 0.0: 0.8542328806703265\n",
      "F1: 0.22452654455138155\n",
      "Area under PR: 0.4714857115569134\n",
      "Area under ROC: 0.5007380707610938\n"
     ]
    }
   ],
   "source": [
    "evaluation(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.26808483235306746\n"
     ]
    }
   ],
   "source": [
    "trainErr = predictionAndLabels.filter(lambda lp: lp[1] != lp[0]).count() / float(data_ny.count()) \n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
